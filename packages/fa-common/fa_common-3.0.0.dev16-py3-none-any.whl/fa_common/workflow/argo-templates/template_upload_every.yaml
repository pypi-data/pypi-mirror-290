name: <<TEMPLATE_NAME>>
container:
  <% if HAS_SECRET %>
  envFrom:
    - secretRef:
        name: <<SECRET_NAME>>
  <% endif %>
  args:
  - |-
    <% if not IS_ERR_TOLER %>
    set -e;
    <% endif %>

    upload_copy_paths=$(echo '{{inputs.parameters.upload-copy-paths}}' | jq -r '.[]');
    selected_upload_objs=$(echo '{{inputs.parameters.selected-upload-objects}}' | jq -r '.[]');

    if [ -z {{inputs.parameters.upload-custom-path}} ]; then
    targetPath={{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{inputs.parameters.pod-name}};
    else
    targetPath={{inputs.parameters.upload-custom-path}}
    fi

    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    # Write the credentials JSON to a temporary file from the environment variable
    echo "$<<SECRET_KEY>>" > /tmp/firebase.json
    export GOOGLE_APPLICATION_CREDENTIALS=/tmp/firebase.json

    function copy_logs {
      gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
      gsutil -m cp -r /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log;
    };
    trap copy_logs EXIT;

    gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS

    if [ -z "${selected_upload_objs[*]}" ]; then
    gsutil -m cp -r /tmp/output/** $targetPath;
    else
    for file in "${selected_files[@]}"; do
        gsutil -m cp -r "${outPath}/${file}" "${targetPath}/"
    done
    fi

    <% if RUN_LOGGING %>gsutil -m cp -r /tmp/log/main.log {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{inputs.parameters.pod-name}}/main.log;<% endif %>
    for p in $upload_copy_paths; do
      gsutil -m cp -r  $targetPath/** $p;  # cloud-2-cloud copy
    done
    <% endif %>


    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    # Check if 's3' is not in the STORAGE_ENDPOINT_URL and set it if absent
    if [[ ! "<<STORAGE_ENDPOINT_URL>>" == *"s3"* ]]; then
      echo "Changing AWS_ENDPOINT_URL to: https://<<STORAGE_ENDPOINT_URL>> "
      export AWS_ENDPOINT_URL="https://<<STORAGE_ENDPOINT_URL>>"
    fi

    function copy_logs {
      aws s3 cp /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log;
    };
    trap copy_logs EXIT;

    if [ -z "${selected_upload_objs[*]}" ]; then
    aws s3 cp /tmp/output/ $targetPath/ --recursive;
    else
    for file in "${selected_files[@]}"; do
        # Determine if it's a directory or a file for conditional recursive flag
        if [ -d "${outPath}/${file}" ]; then
            # If it's a directory, use recursive copy
            aws s3 cp "${outPath}/${file}" "${targetPath}/${file}" --recursive
        else
            # If it's a file, copy it normally
            aws s3 cp "${outPath}/${file}" "${targetPath}/${file}"
        fi
    done
    fi
    <% if RUN_LOGGING %>aws s3 cp /tmp/log/main.log {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{inputs.parameters.pod-name}}/main.log;<% endif %>
    <% endif %>

    for path in $upload_copy_paths; do
      aws s3 cp $targetPath/ $path --recursive;
    done

    # <% if UPLOAD_LOGGING %>copy_logs;<% endif %>
  command:
  - /bin/bash
  - -c
  image: << CLOUD_BASE_IMAGE >>
inputs:
  artifacts:
  - name: output-files
    path: /tmp/output/
    <% if RUN_LOGGING %>
  - name: main-logs
    s3:
      key: '{{workflow.name}}/{{inputs.parameters.pod-name}}/main.log'
    path: /tmp/log/main.log
    <% endif %>
  parameters:
  - name: pod-name
  - name: task-name
  - name: upload-base-path
  - name: upload-copy-paths
  - name: upload-custom-path
  - name: selected-upload-objects
