# This template uses a single node to download, run, and upload results
name: run-template
volumes:
  <% for secret in MOUNT_SECRETS %>
  - name: secret-volume-<< loop.index >>
    secret:
      secretName: << secret.name >>
  <% endfor %>
container:
  envFrom:
    <% for ENV_SECRET in ENV_SECRETS %>
    - secretRef:
        name: << ENV_SECRET >>
    <% endfor %>
    <% for ENV_CONFIG in ENV_CONFIGS %>
    - configMapRef:
        name: << ENV_CONFIG >>
    <% endfor %>
  volumeMounts:
      <% for secret in MOUNT_SECRETS %>
      - name: secret-volume-<< loop.index >>
        mountPath: << secret.mount_path >>
      <% endfor %>
  env:
    <% for key, value in ENV_VARS.items() %>
      - name: << key >>
        value: << value >>
    <% endfor %>
  args:
  - |-
    <% if not IS_ERR_TOLER %>
    set -e;
    <% endif %>
    echo Step-Pod: "{{pod.name}}";

    trap 'copy_download_logs; copy_artifacts;' EXIT;

    # PRE - COMMAND
    # =============
    <<APP_PRE_COMMAND>>;

    # CHECK REQUIRED PACKAGE DEPENDENCIES
    # ===================================
    if ! command -v curl &> /dev/null; then
    echo "curl is not installed.";
    apt-get update && apt-get install -y curl;
    fi

    if ! command -v jq &> /dev/null; then
    echo "jq is not installed.";
    apt-get update && apt-get install -y jq;
    fi

    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    if ! command -v aws &> /dev/null; then
    echo "Installing AWS CLI v2...";
    apt-get update && apt-get install -y unzip;
    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip";
    unzip awscliv2.zip;
    ./aws/install;
    rm -rf awscliv2.zip ./aws;
    fi
    <% endif %>

    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    if ! command -v gsutil &> /dev/null; then
    echo "Installing gsutil via Google Cloud SDK...";
    echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list;
    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -;
    apt-get update && apt-get install -y google-cloud-sdk;
    fi
    <% endif %>


    # CONFIGURE IN/OUT PATHS
    # ======================
    echo Configuring in/out paths;
    path="<<APP_INPUT_PATH>>";
    outPath=<<APP_OUTPUT_PATH>>;
    if [ -d "$path" ]; then
      echo "The path exists."
    else
      echo "The path does not exist. Creating the path now..."
      mkdir -p "$path"

      if [ $? -eq 0 ]; then
        echo "Path successfully created."
      else
        echo "Failed to create path."
      fi
    fi;

    function copy_download_logs {
        echo "No files to download"
    };

    # CONFIGURE GCP STORAGE - DOWNLOAD
    # ================================
    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    echo Configuring gcp cloud storage to download input files...

    echo "$<<SECRET_KEY>>" > /tmp/firebase.json
    export GOOGLE_APPLICATION_CREDENTIALS=/tmp/firebase.json

    function copy_download_logs {
      <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=$(GOOGLE_APPLICATION_CREDENTIALS)<% endif %>
      gsutil -m cp -r /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
    };

    <% if HAS_SECRET %>gcloud auth activate-service-account --key-file=$(GOOGLE_APPLICATION_CREDENTIALS);<% endif %>
    FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
    for public_url in "${FILES[@]}"; do
    file=$(basename $public_url)
    gsutil cp $public_url $path/$file;
    done
    <% endif %>

    # CONFIGURE S3 STORAGE - DOWNLOAD
    # ===============================
    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    echo Configuring s3 cloud storage to download input files...

    if [[ ! "<<STORAGE_ENDPOINT_URL>>" == *"s3"* ]]; then
      echo "Changing AWS_ENDPOINT_URL to: https://<<STORAGE_ENDPOINT_URL>> "
      export AWS_ENDPOINT_URL="https://<<STORAGE_ENDPOINT_URL>>"
    fi

    function copy_download_logs {
      aws s3 cp /var/run/argo/ctr/main/combined {{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}}/main.log
    };

    FILES=($(echo '{{inputs.parameters.files-json}}' | jq -r '.[] | .public_url'))
    for public_url in "${FILES[@]}"; do
    file=$(basename $public_url)
    aws s3 cp $public_url $path/$file;
    done
    <% endif %>

    # HANDLE ARTIFACTS
    # ================
    function copy_artifacts {
      mkdir -p /tmp/output/;

      outPath=<<APP_OUTPUT_PATH>>
      if [ -d "$outPath" ]; then
        cp -r $outPath/* /tmp/output/;
      else
        echo Output path: $outPath did not exist.
      fi
    }


    # INPUT PARAMS: param.json
    # ====================================
    echo {{inputs.parameters.JOB_PARAMETERS}} > $path/param.json;

    # MAIN COMMAND
    # ============
    <<APP_MAIN_COMMAND>>;

    # POST COMMAND
    # ============
    <<APP_POST_COMMAND>>;

    <% if HAS_UPLOAD %>

    <% if JOB.uploads.is_custom_upload %>
    targetPath=<<JOB.uploads.custom_path_uri>>;
    <% else %>
    targetPath={{inputs.parameters.upload-base-path}}/{{workflow.name}}/{{pod.name}};
    <% endif %>

    upload_copy_paths=$(echo '{{inputs.parameters.upload-copy-paths}}' | jq -r '.[]');
    selected_files=(<% for file in JOB.uploads.selected_outputs %>"<< file >>"<% if not loop.last %> <% endif %><% endfor %>);

    <% if STORAGE_TYPE == STORAGE_ENUM.FIREBASE_STORAGE %>
    echo Uploading results to GCP storage...

    <% if JOB.uploads.has_all_outputs %>
    gsutil -m cp -r $outPath/** $targetPath;
    <% else %>
    for file in "${selected_files[@]}"; do
        gsutil -m cp -r "${outPath}/${file}" "${targetPath}/"
    done
    <% endif %>

    for p in $upload_copy_paths; do
      gsutil -m cp -r  $targetPath/** $p;  # cloud-2-cloud copy
    done
    <% endif %>

    <% if STORAGE_TYPE == STORAGE_ENUM.MINIO %>
    echo Uploading results to S3 storage...

    <% if JOB.uploads.has_all_outputs %>
    aws s3 cp $outPath/ $targetPath/ --recursive;
    <% else %>
    for file in "${selected_files[@]}"; do
        # Determine if it's a directory or a file for conditional recursive flag
        if [ -d "${outPath}/${file}" ]; then
            # If it's a directory, use recursive copy
            aws s3 cp "${outPath}/${file}" "${targetPath}/${file}" --recursive
        else
            # If it's a file, copy it normally
            aws s3 cp "${outPath}/${file}" "${targetPath}/${file}"
        fi
    done
    <% endif %>

    for p in $upload_copy_paths; do
      aws s3 cp $targetPath/ $p --recursive;  # cloud-2-cloud copy
    done
    <% endif %>
    <% endif %>

  command:
  - /bin/bash
  - -c
  resources:
    requests:
      memory: <<MEM_REQ>>
      cpu: <<CPU_REQ>>
    limits:
      memory: <<MEM_LIM>>
      cpu: <<CPU_LIM>>
  image: <<IMAGE_URL>>
  imagePullPolicy: <<IMG_PULL_POLICY>>
inputs:
  parameters:
  - name: files-json
  - name: upload-base-path
  - name: JOB_PARAMETERS
  - name: upload-copy-paths
  <% for i in range(MAX_NUM) %>
  - name: dep-art-loc-<<i>>
    default: no-location
  <% endfor %>
  <% for i in range(MAX_NUM) %>
  - name: dep-art-<<i>>
    optional: true
    path: <<APP_INPUT_PATH>>/{{inputs.parameters.dep-art-loc-<<i>>}}/
  <% endfor %>
outputs:
  artifacts:
  - name: output-files
    path: /tmp/output/
  parameters:
  - name: pod-name
    value: '{{pod.name}}'
