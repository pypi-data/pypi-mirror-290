import re
import emoji
import jieba

# ç¤ºä¾‹åœç”¨è¯åˆ—è¡¨ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
stopwords = set([
    "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦",
    "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"
])


def preprocess_text(text):
    # è‹±æ–‡æ ‡ç‚¹è½¬ä¸­æ–‡æ ‡ç‚¹
    punctuation_map = {
        ',': 'ï¼Œ',
        '.': 'ã€‚',
        '!': 'ï¼',
        '?': 'ï¼Ÿ',
        ';': 'ï¼›',
        ':': 'ï¼š',
        '"': 'â€œ',
        "'": 'â€˜',
        '(': 'ï¼ˆ',
        ')': 'ï¼‰',
        '[': 'ã€',
        ']': 'ã€‘',
        '{': 'ã€Š',
        '}': 'ã€‹',
    }
    for eng_punct, zh_punct in punctuation_map.items():
        text = text.replace(eng_punct, zh_punct)

    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<.*?>', '', text)

    # å»é™¤URL
    text = re.sub(r'http[s]?://\S+', '', text)

    # å»é™¤è¡¨æƒ…ç¬¦å·
    text = emoji.replace_emoji(text, replace='')

    # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·

    # å»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def remove_stopwords(words):
    # å»é™¤åœç”¨è¯
    return [word for word in words if word not in stopwords]


def segment_and_clean(text):
    # åˆ†è¯
    words = jieba.lcut(text)
    # å»é™¤åœç”¨è¯
    clean_words = remove_stopwords(words)
    return clean_words


# ç¤ºä¾‹æ–‡æœ¬
sample_text = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬ï¼ŒåŒ…å«HTMLæ ‡ç­¾<a href='https://example.com'>é“¾æ¥</a>ã€è¡¨æƒ…ğŸ˜Šå’Œç‰¹æ®Šå­—ç¬¦ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰ã€‚"

# é¢„å¤„ç†æ–‡æœ¬
cleaned_text = preprocess_text(sample_text)
print("é¢„å¤„ç†åçš„æ–‡æœ¬ï¼š", cleaned_text)

# åˆ†è¯å¹¶å»é™¤åœç”¨è¯
clean_words = segment_and_clean(cleaned_text)
print("åˆ†è¯å¹¶å»é™¤åœç”¨è¯åçš„è¯è¯­ï¼š", clean_words)
