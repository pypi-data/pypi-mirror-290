"""Each .grib file in the HRRR dataset contains dozens or hundreds of distinct variables
that represent data along several dimensions. The inventory files published by NOAA are
useful for the human-readable descriptions, but more reliable inventory dataframes can
be generated by reading the sidecar .grib2.idx files.

The functions in this module generate the metadata required to define the coordinates
along the forecast_valid x level dimensions on which specific variables have data. These
dataframes are used to populate the datacube extension metadata for each collection.

The dimensions of interest are:
1. forecast_valid: either the average, minimum, maximum, or accumulated value for a
    specific time range, e.g. 3-4 hours, 0-1 day, etc.
    For forecast hour 0, the level is "analysis"
2. level: the models generate predictions of many of the variables for various levels
    in the atmosphere, e.g. 0-9000 ft, cloud surface, top of atmosphere, etc.
"""

import logging
import multiprocessing as mp
from datetime import datetime, timedelta
from io import StringIO
from pathlib import Path
from typing import Optional, Union

import httpx
import pandas as pd

from stactools.noaa_hrrr.constants import (
    BYTE_SIZE,
    DESCRIPTION,
    FORECAST_HOUR,
    FORECAST_VALID,
    GRIB_MESSAGE,
    LEVEL,
    REFERENCE_TIME,
    START_BYTE,
    UNIT,
    VARIABLE,
)
from stactools.noaa_hrrr.metadata import (
    CLOUD_PROVIDER_CONFIGS,
    DATA_DIR,
    PRODUCT_CONFIGS,
    REGION_CONFIGS,
    CloudProvider,
    ForecastCycleType,
    ForecastHourSet,
    Product,
    Region,
)

VARIABLE_DESCRIPTIONS_CSV_GZ = "variable_descriptions.csv.gz"

INVENTORY_CSV_GZ_FORMAT = "__".join(
    [
        "inventory",
        "{region}",
        "{product}.csv.gz",
    ]
)

INVENTORY_COLS = [
    GRIB_MESSAGE,
    VARIABLE,
    LEVEL,
    FORECAST_VALID,
]
DESCRIPTION_COLS = [
    DESCRIPTION,
    UNIT,
]

# URLs for the published inventory for each region/product/forecast_hour_set from NOAA
# https://www.nco.ncep.noaa.gov/pmb/products/hrrr/#CO
NOAA_INVENTORY_URLS = {
    (
        Region.conus,
        Product.prs,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf00.grib2.shtml",
    (
        Region.conus,
        Product.prs,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf02.grib2.shtml",
    (
        Region.conus,
        Product.nat,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf00.grib2.shtml",
    (
        Region.conus,
        Product.nat,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf02.grib2.shtml",
    (
        Region.conus,
        Product.sfc,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf00.grib2.shtml",
    (
        Region.conus,
        Product.sfc,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf02.grib2.shtml",
    (
        Region.conus,
        Product.subh,
        ForecastHourSet.FH00,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf00.grib2.shtml",
    (
        Region.conus,
        Product.subh,
        ForecastHourSet.FH01_18,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf02.grib2.shtml",
    (
        Region.alaska,
        Product.prs,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.prs,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf02.ak.grib2.shtml",
    (
        Region.alaska,
        Product.nat,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.nat,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf02.ak.grib2.shtml",
    (
        Region.alaska,
        Product.sfc,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.sfc,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf02.ak.grib2.shtml",
    (
        Region.alaska,
        Product.subh,
        ForecastHourSet.FH00,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.subh,
        ForecastHourSet.FH01_18,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf02.ak.grib2.shtml",
}

# dummy datetime for generating inventory csv files
dummy_datetime = datetime(year=2024, month=5, day=1)


def read_inventory_df(
    region: Region,
    product: Product,
    forecast_hour: Optional[int] = None,
) -> pd.DataFrame:
    """Read variable inventory DataFrame

    Load the inventory of variables with descriptions, units, and level/forecast time
    dimension values for a given region/product/forecast_hour_set/forecast_cycle_type
    """
    inventory_df = pd.read_csv(
        DATA_DIR
        / INVENTORY_CSV_GZ_FORMAT.format(
            region=region.value,
            product=product.value,
        ),
        index_col=FORECAST_HOUR,
    ).replace(pd.NA, None)

    # optionally subset down to a specific forecast hour
    if forecast_hour is not None:
        inventory_df = inventory_df.loc[forecast_hour]

    return inventory_df


def read_variable_description_df() -> pd.DataFrame:
    """Read the variable description dataframe"""
    return pd.read_csv(
        DATA_DIR / VARIABLE_DESCRIPTIONS_CSV_GZ,
    )


# Define custom exceptions
class NotFoundError(Exception):
    pass


def load_idx(
    region: Region,
    product: Product,
    cloud_provider: CloudProvider,
    reference_datetime: datetime,
    forecast_hour: int,
) -> StringIO:
    """Load the contents of a .idx file in a format that can be read by pandas"""

    region_config = REGION_CONFIGS[region]
    cloud_provider_config = CLOUD_PROVIDER_CONFIGS[cloud_provider]

    idx_url = cloud_provider_config.url_base + region_config.format_grib_url(
        product=product,
        reference_datetime=reference_datetime,
        forecast_hour=forecast_hour,
        idx=True,
    )

    read_this_idx = None

    response = httpx.get(idx_url, timeout=20)

    try:
        response.raise_for_status()  # Raise an HTTPStatusError for 4xx/5xx status codes
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            response.close()
            raise NotFoundError(f"404 Not Found: {idx_url}")
        else:
            response.close()
            raise e

    read_this_idx = StringIO(response.text)
    response.close()

    return read_this_idx


def read_idx(idx: Union[str, StringIO]) -> pd.DataFrame:
    """Read the contents of a .idx file, heavily cribbed from Herbie"""

    df = pd.read_csv(
        idx,
        sep=":",
        names=[
            GRIB_MESSAGE,
            START_BYTE,
            REFERENCE_TIME,
            VARIABLE,
            LEVEL,
            FORECAST_VALID,
            "?",
            "??",
            "???",
        ],
    )

    # Format the DataFrame
    df[REFERENCE_TIME] = pd.to_datetime(df.reference_time, format="d=%Y%m%d%H")
    df[START_BYTE] = df[START_BYTE].astype(int)
    df[BYTE_SIZE] = (df[START_BYTE].shift(-1) - df[START_BYTE]).astype(pd.Int64Dtype())
    df = df.reindex(
        columns=[
            GRIB_MESSAGE,
            START_BYTE,
            BYTE_SIZE,
            REFERENCE_TIME,
            VARIABLE,
            LEVEL,
            FORECAST_VALID,
            "?",
            "??",
            "???",
        ]
    ).dropna(how="all", axis=1)

    # load the dataframe with units/descriptions for each variable
    variable_description_df = read_variable_description_df()

    df = df.merge(
        variable_description_df[[VARIABLE, DESCRIPTION, UNIT]],
        how="left",
        on=VARIABLE,
    ).replace(pd.NA, None)

    return df


def generate_single_inventory_df(
    region: Region,
    product: Product,
    cycle_run_hour: int,
    forecast_hour: int,
) -> pd.DataFrame:
    """Generate a single inventory DataFrame

    Loads the inventory dataframe from a .idx file"""
    idx_df = read_idx(
        idx=load_idx(
            region=region,
            product=product,
            cloud_provider=CloudProvider.azure,
            reference_datetime=dummy_datetime + timedelta(hours=cycle_run_hour),
            forecast_hour=forecast_hour,
        ),
    )
    out = idx_df.assign(
        region=region.value,
        product=product.value,
        forecast_hour=forecast_hour,
    )[[FORECAST_HOUR] + INVENTORY_COLS]

    assert isinstance(out, pd.DataFrame)

    return out


def generate_variable_descriptions_csv_gz(dest_dir: Path) -> None:
    """Read the NOAA inventory files and create a csv with the units and human-readable
    descriptions of each variable"""
    dfs = []
    for region in Region:
        for product in Product:
            for forecast_hour_set in PRODUCT_CONFIGS[product].forecast_hour_sets:
                # get the variable descriptions from the NOAA inventory tables
                noaa_inventory = pd.read_html(
                    NOAA_INVENTORY_URLS[region, product, forecast_hour_set],
                )[1]

                noaa_inventory[[DESCRIPTION, UNIT]] = noaa_inventory[
                    "Description"
                ].str.extract(r"(.+?) \[(.+?)\]")

                variable_descriptions = (
                    noaa_inventory[["Parameter", DESCRIPTION, UNIT]]
                    .drop_duplicates()
                    .rename(columns={"Parameter": VARIABLE})
                )

                dfs.append(variable_descriptions)

    variable_descriptions_csv_gz = dest_dir / VARIABLE_DESCRIPTIONS_CSV_GZ
    pd.concat(dfs).drop_duplicates().to_csv(variable_descriptions_csv_gz, index=False)

    logging.info(f"Data successfully written to {variable_descriptions_csv_gz}")


def generate_inventory_csv_gzs(dest_dir: Path) -> None:
    """Generate all inventory dataframes and write to .csv.gz files"""
    generate_variable_descriptions_csv_gz(dest_dir)

    variable_descriptions = pd.read_csv(dest_dir / VARIABLE_DESCRIPTIONS_CSV_GZ)
    forecast_cycle_type = ForecastCycleType("extended")
    cycle_run_hour = 0
    for region in Region:
        for product in Product:
            product_config = PRODUCT_CONFIGS[product]
            inventory_dfs = []
            allowed_forecast_hours = list(forecast_cycle_type.generate_forecast_hours())

            for forecast_hour_set in product_config.forecast_hour_sets:
                tasks = []
                for forecast_hour in list(
                    set(forecast_hour_set.generate_forecast_hours())
                    & set(allowed_forecast_hours)
                ):
                    tasks.append(
                        (
                            region,
                            product,
                            cycle_run_hour,
                            forecast_hour,
                        )
                    )

                with mp.Pool() as pool:
                    dfs = pool.starmap(generate_single_inventory_df, tasks)

                inventory_df = pd.concat(dfs)

                n_rows = inventory_df.shape[0]

                # add the variable descriptions
                inventory_df = inventory_df.merge(
                    variable_descriptions, on=VARIABLE, how="left"
                )
                assert inventory_df.shape[0] == n_rows

                inventory_dfs.append(inventory_df)

            inventory_csv_gz = dest_dir / INVENTORY_CSV_GZ_FORMAT.format(
                region=region.value,
                product=product.value,
            )
            pd.concat(inventory_dfs).to_csv(inventory_csv_gz, index=False)

            logging.info(f"Data successfully written to {inventory_csv_gz}")
