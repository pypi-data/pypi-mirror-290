# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['ngs_pipeline_lib',
 'ngs_pipeline_lib.api',
 'ngs_pipeline_lib.base',
 'ngs_pipeline_lib.biotools',
 'ngs_pipeline_lib.cli',
 'ngs_pipeline_lib.tools',
 'ngs_pipeline_lib.tools.quality_control',
 'scripts',
 'scripts.api',
 'scripts.common',
 'scripts.docker',
 'scripts.run',
 'scripts.tests',
 'scripts.tests.models',
 'scripts.tests.utils']

package_data = \
{'': ['*']}

install_requires = \
['boto3>=1.28.52,<2.0.0',
 'clidantic>=0.1.0,<0.2.0',
 'colorama>=0.4.6,<0.5.0',
 'coverage>=7.2.7,<8.0.0',
 'docker>=7.0.0,<8.0.0',
 'fastapi>=0.111.0,<0.112.0',
 'flatdict>=4.0.1,<5.0.0',
 'pgzip>=0.3.5,<0.4.0',
 'pydantic[dotenv]>=1.10.7,<2.0.0',
 'pyjwt>=2.8.0,<3.0.0',
 'pytest-mock>=3.10.0,<4.0.0',
 'structlog>=24.1.0,<25.0.0']

entry_points = \
{'console_scripts': ['ngs-api = scripts.api.main:api',
                     'ngs-build = scripts.docker.main:build',
                     'ngs-push = scripts.docker.main:push',
                     'ngs-run = scripts.run.main:run',
                     'ngs-test = scripts.tests.main:test']}

setup_kwargs = {
    'name': 'ngs-pipeline-lib',
    'version': '5.1.3',
    'description': 'Common code used by every processes in Bio Pipelines',
    'long_description': '# Introduction\n\nProvide : \n\n- A base set of tools and classes to implement BioInfo algorithms\n- Execution context, using the `ngs-run` script\n\n# Installation\n\nAdd the CodeArtifact repository to your pyproject.toml\n```\n[[tool.poetry.source]]\nname = "codeartifact"\nurl = "https://pdx-platform-224016688692.d.codeartifact.eu-west-1.amazonaws.com/pypi/pdx-python-libs/simple/"\nsecondary = true\n```\n\nThen authenticate your local environment to CodeArtifact \n\n```bash\nexport CODEARTIFACT_AUTH_TOKEN=$(aws codeartifact get-authorization-token --domain pdx-platform --query authorizationToken --output text --profile ADX_DEV)\npoetry config http-basic.codeartifact aws $CODEARTIFACT_AUTH_TOKEN\n```\n\n**Note** : The token acquired with AWS is a temporary one. Each time you want to download new packages from the CodeArtifact repository, you may have to re-do the authentication process.\n\nThen, simply add the library to your poetry dependencies.\n\n```bash\npoetry add ngs-pipeline-lib --source codeartifact\n```\n\n# Update\n\nTo update to a newer version of the library : \n```bash\npoetry update ngs-pipeline-lib\n```\n\n> You may need to update your version constraint in the pyproject.toml file \n\n# Getting started\n\nOnce the library has been installed in your project, you can implement your algorithms by extending the `Algorithm` class.\n\nIf you want to add specific inputs to your Algorithm, extend `BaseInputs` (which is a Pydantic Model) and use it as the inputs Type.  \nIn order to adds outputs, extend BaseOutputs and set the `outputs_class` class attribute of your algorithm as this class.  \nIf you have specific inputs or outputs classes, you should also provide them to `Algorithm` when subclassing it. You place them between brackets as shown below, this will help your IDE undestand what kind of object it is dealing with, thus improving the autocompletion and the tooltips. \n\n\n```python\nfrom pydantic import Field\n\nfrom ngs_pipeline_lib.base.algorithm import Algorithm\nfrom ngs_pipeline_lib.base.inputs import BaseInputs\nfrom ngs_pipeline_lib.base.file import JsonFile\nfrom ngs_pipeline_lib.base.outputs import BaseOutputs\n\nclass YourInputs(BaseInputs):\n  your_input: str = Field(description="Description")\n\n\nclass YourOutputs(BaseOutputs):\n  \n  def __init__(self):\n        super().__init__()\n        self.my_own_output = JsonFile(name="my_json_file")\n\nclass YourAlgorithm(Algorithm[YourInputs, YourOutputs]):\n\n    outputs_class = YourOutputs\n\n    def execute_stub(self):\n        ...\n\n    def execute_implementation(self):\n        print(self.inputs.your_input)\n        ...\n\n```\n\n## Enable CLI Mode\n\nTo allow your Algorithm to be executed through a CLI, you must write a file `cli.py` in your `src` folder.\n\n**Example**\n```python\nfrom ngs_pipeline_lib.cli import cli\n\nfrom .DemoAlgorithm import DemoAlgorithm, DemoInputs\n\n\n@cli.command(name="Demo")\ndef run_cli(args: DemoInputs):\n    """Demo Algorithm to demonstrate a basic implementation"""\n    algorithm = DemoAlgorithm(args)\n    algorithm.execute()\n```\n\n> Note : the method name is not important, you can use whatever you want.\n\nThen, you can call your Algorithm with the following command\n\n`poetry run ngs-run --sample-id 1 --text-file.path data/some_text_file.txt`\n\nIf you want to only create the stub output file, add the `--stub` parameter.\n\n## Enable API Mode\n\nTo allow your Algorithm to be executed through an API, you must write a file `api.py` in your `src` folder.\n\n**Example**\n```python\nfrom ngs_pipeline_lib.api import app\n\nfrom .DemoAlgorithm import DemoAlgorithm, DemoInputs\n\n\n@app.post("/")\nasync def run_api(args: DemoInputs):\n    algorithm = DemoAlgorithm(args)\n    algorithm.execute()\n    return algorithm.outputs._outputs.content\n\n```\n\n> Note : the method name is not important, you can use whatever you want.\n\nThen, you can start your Process as an API Service with the following command : \n\n`poetry run ngs-api`\n\nAnd then, you can request your API through HTTP requests, like : \n\n**Curl example**\n```\ncurl -X \'POST\' \\\n  \'http://127.0.0.1:8000/\' \\\n  -H \'accept: application/json\' \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n  "sample_id": "1",\n  "text-file.path": "data/some_text_file.txt",\n  "value": 5,\n  "stub": false\n}\'\n```\n\nThe following settings can be used to customize the API behaviour : \n\n| Settings                   | Environment Variable       | Description                                    | Mandatory ?   | Default value |\n| -------------------------- | -------------------------- | ---------------------------------------------- | ------------- | ------------- |\n| secure                     | API_SECURE                 | Enable the oAuth2 validation on private router | No            | False         |\n| root_path                  | API_ROOT_PATH              | Set a root path to be used when behind a proxy | No            | None          |\n| prefix_path                | API_PREFIX_PATH            | Set a prefix path to be used on every route    | No            | ""            |\n| keycloak_authorization_url | KEYCLOAK_AUTHORIZATION_URL | Set the keycloak oAuth2 auth url               | Yes if secure | None          |\n| keycloak_token_url         | KEYCLOAK_TOKEN_URL         | Set the keycloak oAuth2 token url              | Yes if secure | None          |\n| keycloak_refresh_url       | KEYCLOAK_REFRESH_URL       | Set the keycloak oAuth2 refresh url            | Yes if secure | None          |\n| keycloak_certs_url         | KEYCLOAK_CERTS_URL         | Set the keycloak oAuth2 certs url              | Yes if secure | None          |\n\n### Swagger Documentation\n\nWhen running your Process as an API Service, an auto-generated [Swagger documentation ](http://localhost:8000/docs#/) is available.\n\nFor private routes that needs OAuth2 authentication, you can authenticate using the "Authorize" button at the top of the Swagger page.\nIn the form, you have to set a correct client_id / client_secret from a running keycloak instance. \n\n> **Important** : In the keycloak client settings, ensure the `Valid redirect URIs` contains `http://localhost:8000/*`\n\n## Input files/directories\nInput files and directories can be declared as local paths, in which case it is validated that they are present.\nWhen running locally, in CLI mode, input files can be created/staged manually or by mapping docker volumes.\nWhen running as a nextflow process, nextflow will take care of staging the inputs.\n\nInput files and directories can also be declared as "downloadable" paths. Then, the input needs to either specify the `.path` argument, if the path is staged locally,\nor the `.url` argument, if the path must be downloaded from S3. The PROFILE environment variable is used to initialize the AWS S3 session.\nDownloaded files are staged in a temporary folder, and unstaged (deleted) when the process is finished.\n\n## Output files\nOutput files are by default written in the app\'s working directory. There, they can be picked up by an external process, like nextflow, to be published.\nThe `--publish-dir` input argument is only used to log the external publish location in the `outputs.json`, unless the `--publish` flag is set.\n\nWhen the `--publish` flag is set, the `--publish-dir` input is validated to be either an S3 url, or a creatable local directory. At the end of the process,\nthe output files are published to the publishing location, and deleted from the working directory.\n\n## Implementation Example\nIn `example/` you\'ll find the implementation of a dummy algorithm `DemoAlgorithm`.  \nThis algorithm takes 3 parameters:\n- `value`: an integer \n- `kb`: a path to a knowledge base (a local or S3 folder that contains `info.json` which references other local inputs and/or contains values), here it holds\n  - `value`: an optional float (you can safely remove it from `info.json`)\n  - `json_file`: a path to a json file (local or on S3)\n- `text_file`: a path to a text file  \n\nThe `example/data` folder contains some dummy data to run the algorithm.  \nYou can call it (from within `example/`) using : \n```sh\npoetry run ngs-run --sample-id some_id --text-file.path data/some_text_file.txt --kb.path data/demo_kb\n```\n\nUsing S3 to stage and publish files:\n```shell\nexport PROFILE="<my_aws_profile>"\npoetry run ngs-run --sample-id some_id --text-file.url s3://my_bucket/data/some_text_file.txt --kb.url s3://my_bucket/data/demo_kb --publish --publish-dir s3://my_bucket/publish/some_id/\n```\n\nAdd the `--stub` flag to run the stub instead of the implementation.\n\n# Docker build & push\n\nThis library also includes two utilitary scripts to build & push Docker image : \n\n- ngs-build\n- ngs-push\n\n### Build\n\nThis script accepts the following arguments : \n\n| Short Arg | Long Arg   | Description             | Mandatory ? | Default value |\n| --------- | ---------- | ----------------------- | ----------- | ------------- |\n| -e        | --env-file | Path to env file to use | No          | `.env`        |\n\nThis script accepts the following environment variables as parameters \n\n| ENV VAR               | Description                                                  | Mandatory ? | Default value           |\n| --------------------- | ------------------------------------------------------------ | ----------- | ----------------------- |\n| PROCESS_NAME          | Name of the process                                          | Yes         | --                      |\n| IMAGE_PREFIX          | Prefix used with the process name to create Docker repo name | No          | `ngs-pipeline-process-` |\n| TAG                   | Tag of the image to create                                   | No          | `latest`                |\n| DOCKERFILE            | Relative path to Dockerfile                                  | No          | `Dockerfile`            |\n| PIP_REGISTRY_USERNAME | If needed, username to use for pip auth                      | No          | --                      |\n| PIP_REGISTRY_PASSWORD | If needed, password to use for pip auth                      | No          | --                      |\n\n> Note : the docker context used to build is `.`\n\n### Push\n\nThis script accepts the following arguments : \n\n| Short Arg | Long Arg   | Description             | Mandatory ? | Default value |\n| --------- | ---------- | ----------------------- | ----------- | ------------- |\n| -e        | --env-file | Path to env file to use | No          | `.env`        |\n\nThis script accepts the following environment variables as parameters \n\n| ENV VAR               | Description                                 | Mandatory ? | Defaut value            |\n| --------------------- | ------------------------------------------- | ----------- | ----------------------- |\n| EXTERNAL_REGISTRY_URL | URL of Destination Registry                 | Yes         | --                      |\n| PROCESS_NAME          | Name of the process to push                 | Yes         | --                      |\n| IMAGE_PREFIX          | Prefix used in the process Docker repo name | No          | `ngs-pipeline-process-` |\n| TAG                   | Tag of the image to create                  | No          | `latest`                |\n| DOCKER_USERNAME       | If needed, username to use for docker auth  | No          | --                      |\n| DOCKER_PASSWORD       | If needed, password to use for docker auth  | No          | --                      |\n\n# Test tools\n\nThis library comes with two tools :\n - Integration test, to verify the behaviour of one process\n - E2E test, to verify the workflow of a complete pipeline with one/multiple samples\n\nTo run the integration test : \n\n"""bash\npoetry run ngs-test integration \n"""\n\nTo run the E2E test : \n\n"""bash\npoetry run ngs-test e2e --output-path <<YOUR_PIPELINE_OUTPUT_DIR>>  --scenario-file <<SCENARIO_PATH>>\n"""\n\n## End-To-End Test\n\nThis tool will do the following : \n- Load specified test scenario\n  - Can be local or S3 path\n- Load specified pipeline run :\n  - Import the trace file\n  - Import the hashed_id mapping file\n  - Import the execution.json file\n  - Explore all published files per sample & process\n- Compare scenario and run\n  - Check samples consistency (missing or extra)\n  - Check task consistency for each sample (missing or extra, but also status)\n  - Check published files for each task on each samples (missing or extra)\n\n> Note : no validation is done on the file\'s content, only its presence. Please use integration tests for this purpose.\n\n### Execution params\n| Arg             | Description                                                     | Mandatory ? | Default value |\n| --------------- | --------------------------------------------------------------- | ----------- | ------------- |\n| --output-path   | The pipeline run to verify                                      | Yes         |               |\n| --scenario-file | The scenario to use, containing expected samples, tasks & files | Yes         |               |\n\n\n### Settings\n\nAll these settings are primarily loaded from the `.env` file. Ensure to have that file before running E2E tests.\nThey can be overridden by manually defining environment variables before launching `ngs-test integration`.\n\n| Environment Variable    | Description                                                       | Mandatory ? | Default value          |\n| ----------------------- | ----------------------------------------------------------------- | ----------- | ---------------------- |\n| PROFILE                 | AWS Profile to use when connecting to S3 through boto3            | Yes         | ADX_DEV                |\n| TEST_OUTPUT_FOLDER      | Local path where to store results and expected output of pipeline | No          | tests/e2e/outputs      |\n| NEXTFLOW_TRACE_FILE     | Trace file path to look for in the output_folder                  | Yes         | trace.txt              |\n| NEXTFLOW_HASHED_ID_FILE | sample_to_hashed_id file path to look for in the output_folder    | No          | sample_to_hash_map.tsv |\n| NEXTFLOW_EXECUTION_FILE | Execution file path to look for in the output_folder              | No          | execution.json         |\n\n## Integration Test\n\nThis tool must be used within a process project, containing a `.env` file with standard envvar (image_prefix, process_name etc...)\nWhen used, this tool will do the following : \n- Load all scenarios\n  - Search for scenario in a dedicated folder (by default : tests/integration/scenarios)\n  - Can be filtered with the param `name_filter`\n- For each scenario\n  - Run the process image with the specified inputs\n  - Extract output from container\n  - Download expected output\n  - Compare outputs\n\n### Execution params\n| Arg                            | Description                                                                                              | Mandatory ? | Default value |\n| ------------------------------ | -------------------------------------------------------------------------------------------------------- | ----------- | ------------- |\n| --name-filter                  | Specify some filter on scenario name. Can be used multiple times (logical `AND` applied between filters) | No          |               |\n| --post-clean / --no-post-clean | Flag to enable/disable cleaning of input and output files after test completion.                         | No          | True          |\n\n\n### Settings\n\nAll these settings are primarly loaded from `.env` file. Ensure to have that file before running integration tests.\nThey can be overriden by defining manually environment variables before launching `ngs-test integration`.\n\n| Environment Variable        | Description                                                                                | Mandatory ? | Default value               |\n| --------------------------- | ------------------------------------------------------------------------------------------ | ----------- | --------------------------- |\n| REMOTE_DOCKER_REPO          | Docker repository to use when launching process container                                  | Yes         |                             |\n| IMAGE_PREFIX                | Combined with PROCESS_NAME to set the docker image to use when launching process container | Yes         |                             |\n| PROCESS_NAME                | Combined with IMAGE_PREFIX to set the docker image to use when launching process container | Yes         |                             |\n| TAG                         | Docker tag to use when launching process container                                         | Yes         |                             |\n| PROFILE                     | AWS Profile to use when connecting to S3/ECR through boto3                                 | Yes         |                             |\n| TEST_SCENARIOS_FOLDER       | Local path where to look for scenarios                                                     | No          | tests/integration/scenarios |\n| TEST_OUTPUT_FOLDER          | Local path where to store results and expected output of process                           | No          | tests/integration/outputs   |\n| TEST_LOCAL_INPUT_FOLDER     | Local path where to put input files (downloaded from S3)                                   | No          | tests/integration/inputs    |\n| TEST_CONFIGURATION_FILENAME | Filename to look for when loading a scenario                                               | No          | test.json                   |\n| VERBOSE                     | Enable debug logs                                                                          | No          | False                       |\n| JSON_LOGGER                 | Enable JSON formatting of logs                                                             | No          | False                       |\n| LOG_FILE                    | Path to current log file                                                                   | No          | None                        |\n\n# Best Practices\n\nWhen implementing your process, please refer to the [guidelines](./docs/GUIDELINES.md) documentation.',
    'author': 'Vincent ROHOU',
    'author_email': 'vincent.rohou@ext.biomerieux.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'entry_points': entry_points,
    'python_requires': '>=3.11,<4.0',
}


setup(**setup_kwargs)
